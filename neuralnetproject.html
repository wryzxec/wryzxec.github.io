<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lucas Bruckbauer</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <div class="container">
        <div class="home-link">
            <a href="index.html">Home</a>
        </div>
        <h1>Neural Network from Scratch</h1>
        <p>Building a Neural Network in Python, without using any external ML frameworks.</p>
        <div class="project-link">
            <a href="https://github.com/wryzxec/MNIST_NeuralNet" target="_blank">GitHub Repo</a>
        </div>
        
        <div class="spacer"></div>
        <div class="project-subtitle">Setting the Goal</div>
        <p class="project-text">
            The goal of this project was to build a simple neural network from scratch, 
            capaple of classifying images of handwritten digits with high accuracy.
            The main purpose of building it from scratch was to better my own understanding of 
            the stuff that happens 'under the hood' with popular ML frameworks 
            like TensorFlow and PyTorch.
        </p>

        <div class="project-subtitle">The MNIST Dataset</div>
        <p class="project-text">
            The MNIST dataset is a dataset of 70,000 28x28 grayscale images of handwritten digits.
            Each image contains a singular digit from 0-9. 
            We separate these images into two sets: the training set and the test set.
            We allocate 60,000 images for training and 10,000 images for testing the networks performance.
        </p>
        <img class="image" src="./assets/neuralnetproject/MNIST_examples.png" alt="MNIST Examples">
        <p class="project-text">
            Many of the digits in the dataset are poorly drawn. For example they are not always clear [example 2] 
            and sometimes written at angles [example 7].
        </p>
        <div class="project-subtitle">
        The Network (An Overview)
        </div>
        <p class="project-text">
            The type of Neural Network implemented in this project is a feedforward neural network.
            This means that the data flows in one direction, from the input layer to the output layer.
            In brief terms, the network is made up of layers of neurons, each of which is connected to the neurons in the next layer.
            Each neuron has a weight and a bias, which are used to calculate the output of the neuron. The weights and biases are adjusted during training and is 
            what allows the network to learn.
        </p>
        <p class="project-text">
            Here is the process, split into distinct steps:
        </p>
        <div class="project-list">
            <ol>
                <li><p class="bold-enhance">Initialisation</p>: Give the neurons in the network starting weights and biases.</li>
                <li><p class="bold-enhance">Forward Propagation</p>: Pass the input data through the network layers and obtain predictions.</li>
                <li><p class="bold-enhance">Compute Loss</p>: Pass the predictions through a loss function to measure the error of the network.</li>
                <li><p class="bold-enhance">Back Propagation</p>: Using partial derivatives, compute the gradient of the loss function with respect to each weight and bias in the network.</li>
                <li><p class="bold-enhance">Update Parameters</p>: Update the weights and biases in the direction that minimises the loss, using the Gradient Descent algorithm.</li>
                <li><p class="bold-enhance">Repeat</p>: Carry out steps 2-5 until the network converges sufficiently.</li>
            </ol>
        </div>
        <p class="project-text">
            To better understand the overal structure of the network, here is a diagram of the network architecture. It is important to note 
            that the real network used in this project has 784 input neurons, each representing a single pixel of an image,
            and a variable number of layers. This diagram is a simplified version. The output layer, however, must remain a fixed size of 10 neurons. 
            Each of the 10 neurons in the output corresponds to a probability that the input image is the digit represented by that neuron (0-9).
        </p>
        <img class="image" src="./assets/neuralnetproject/neural_network_architecture.png" alt="Neural Network Architecture" /> 
        <div class="project-subtitle">
            The Network (at it's Core)
        </div>
        <div class="project-subsubtitle">
            Activation Functions
        </div>
        <div class="project-text">
            Activation functions transform each neuron's input to produce a corresponding output. 
            They also introduce non-linearity into the network, enabling it to learn complex patterns in the data. 
            Without activation functions, the network would behave like a simple linear model. 
            For this network, we use two activation functions: ReLU and Softmax, defined as follows:
        </div>
        \[
        \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}
        \]
        \[
        \text{ReLU}(z) = 
        \begin{cases} 
        0 & \text{if } z \leq 0 \\ 
        z & \text{if } z > 0 
        \end{cases}
        \]
        <div class="project-text">
            The ReLU function is used in the hidden layers of the network, while the Softmax function is used in the output layer. The Softmax 
            function is used to convert the output of the network into probabilities.
        </div>
        <div class="project-subsubtitle">
            Loss Function
        </div>
        <div class="project-text">
            For this Network we are using the Categorical Cross-Entropy Loss function. This function is used to measure the error of the network.
            It is defined as follows: 
        </div>
        \[
        L(y, \hat{y}) = -\sum_{i=1}^{n} y_i \cdot \log(\hat{y}_i)
        \]
        <div class="project-text">
            Here \(\hat{y}\) is the true label of the input data and \(y\) is the predicted label.
            \(n\) is the number of classes in the dataset, which in this case is 10.
        </div>
        <div class="project-subsubtitle">
            Gradient Descent
        </div>
        <div class="project-text">
            Gradient Descent is an optimisation algorithm used to minimise the loss function of the network.
            It works by updating the weights and biases of the network, opposite to the direction of
            the gradient of the loss function, with respect to the weights and biases.
        </div>
        <div class="project-text">
            This is an iterative process and can be defined by an update rule as follows:
        </div>
        \[
        \begin{align*}
        W_{\text{new}} &:= W_{\text{old}} - \alpha \frac{\partial L}{\partial W}\\
        b_{\text{new}} &:= b_{\text{old}} - \alpha \frac{\partial L}{\partial b}
        \end{align*}
        \]
        <div class="project-text">
            Here \(W\) and \(b\) are the weights and biases of the network,
            \(L\) is the loss function, and \(\alpha\) is the learning rate. The learning rate is a hyperparameter
            (a parameter that is set before training begins)
            which determines how much the weights and biases are updated with each iteration.
        </div>

        <div class="project-subsubtitle">
            Back Propagation
        </div>

        <div class="project-text">
            Back Propagation is how the network learns. It is the process of calculating the gradient of the loss function with respect to each weight and bias in the network.
            There are many equations involved an they are dependent on the loss and activation functions used in the network.
            One thing to notice is that each equation is dependent on a previous one, which is an application of the chain rule in calculus.
        </div>
        \[
        \begin{align*}
        \frac{\partial L}{\partial z_k} &= A_k - y_k\\
        \\
        \frac{\partial L}{\partial W_k} &= \frac{1}{m} \left( A_{\text{k-1}}^T \cdot \frac{\partial L}{\partial z_k} \right)\\
        \\
        \frac{\partial L}{\partial b_k} &= \frac{1}{m}\sum_{k=1}^{m} \frac{\partial L}{\partial z_k}
        \end{align*}
        \]

        <div class="project-text">
            For the ReLU layer, the equations for the gradients are as follows:
        </div>

        \[
        \begin{align*}
        \frac{\partial L}{\partial Z_{k-1}} &= \left( W_k^T \cdot \frac{\partial L}{\partial Z_k} \right) \cdot \text{ReLU}'(Z_{k-1})\\
        \\
        \frac{\partial L}{\partial W_k} &= \frac{1}{m} \left( A_{\text{k-1}}^T \cdot \frac{\partial L}{\partial z_k} \right)\\
        \\
        \frac{\partial L}{\partial b_k} &= \frac{1}{m}\sum_{k=1}^{m} \frac{\partial L}{\partial z_k}
        \end{align*}
        \]

        <div class="project-subtitle">
            Optimising the Network
        </div>

        <div class="project-subsubtitle">
            He Initialisation
        </div>

        <div class="project-text">
            It is important to correctly initialise weights in a neural network.
            If weights are too small or too large, the network will not learn effectively.

            He Initialisation reduces the variance of weights between layers, preventing gradients from exploding or vanishing. 
            It is defined as follows:
        </div>
        \[
        W \sim N(0, \frac{2}{n_{in}})
        \]

        <div class="project-text">
            This denotes a normal distribution with a mean of 0 and a standard deviation of
            \(
            \sqrt{\frac{2}{n_{in}}}
            \),
            where
            \(
            n_{in}
            \)
            is the number of neurons in the previous layer.
        </div>
        <div class="project-text">
            Note that whilst biases are often initialised to 0, weights cannot be. This can be explained by looking at the equation for the output of a neuron:
        </div>
        \[
            z = W \cdot X + b
        \]
        <div class="project-text">
            We can see that if the weights are initialised to 0, then
            \(W \cdot X = 0.\) This means that the neuron output does not depend on the input \(X\) and so all neurons will output the same value.
            This will result in the network failing.
        </div>

        <div class="project-subsubtitle">
            Mini-Batch Gradient Descent
        </div>

        <div class="project-text">
            Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches. These
            batches are randomly sampled from the training set.
            When data is processed in these small batches it means that the loss calculation as well as weight and bias updates are carried out with each mini-batch.
            This is unlike batch gradient descent where the training set is processed as a whole unit.
        </div>
        <div class="project-text">
            This frequent updating can help the model find better generalisation since it prevents the model from being stuck in local minima or saddle points for too long.
        </div>
        <div class="project-subsubtitle">
            Gradient Descent with Momentum
        </div>
        <div class="project-text">
            Gradient Descent with Momentum builds on the Gradient Descent algorithm. 
            It calculates an exponentially weighted average of the gradients which is used to update the weights and biases. 
            This allows for the network to converge much faster to the optimal solution as it dampens oscillations in the gradient.
        </div>
        <div class="project-text">
            The update rule for Gradient Descent with Momentum is as follows:
        </div>
        \[
        \begin{align*}
        V_{\delta w} &:= \beta \cdot V_{\delta w} + (1-\beta)\cdot \delta w\\
        V_{\delta b} &:= \beta \cdot V_{\delta b} + (1-\beta)\cdot \delta b
        \end{align*}
        \]

        \[
        \begin{align*}
        w &:= w-\alpha V_{\delta w}\\
        b &:= b-\alpha V_{\delta b}
        \end{align*}
        \]

        <div class="project-text">
            Here \(\delta \) is the momentum term, which is a hyperparameter that determines how much the previous gradients affect the current update.
        </div>

        <div class="project-subtitle">
            The Results
        </div>

        <div class="project-text">
            Here are some of the results from training the network with a variety of different settings. Larger networks with more neurons and layers tend to perform better, but 
            also take longer to train. It's also important to note that just increasing the time of training does not always lead to better results. Train the network
            for too long and the network will begin to overfit the training data and not generalise well to new data.
            After a lot of tweaking and testing, the network managed to achieve an accuracy of 98.42% on the test set!
        </div>

        <table class="styled-table">
            <thead>
                <tr>
                    <th>Epochs</th>
                    <th>Mini-Batch Size</th>
                    <th>Neurons (Layer 1/.../Layer n)</th>
                    <th>Learning Rate (\(\alpha\))</th>
                    <th>Momentum Applied</th>
                    <th>Accuracy (%)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>20</td>
                    <td>128</td>
                    <td>200/100/25/10</td>
                    <td>0.5</td>
                    <td>True</td>
                    <td>98.42</td>
                </tr>
                <tr>
                    <td>20</td>
                    <td>128</td>
                    <td>100/50/10</td>
                    <td>0.5</td>
                    <td>True</td>
                    <td>98.14</td>
                </tr>
                <tr>
                    <td>20</td>
                    <td>128</td>
                    <td>100/50/10</td>
                    <td>0.5</td>
                    <td>False</td>
                    <td>97.51</td>
                </tr>
                <tr>
                    <td>20</td>
                    <td>60000</td>
                    <td>100/50/10</td>
                    <td>0.5</td>
                    <td>True</td>
                    <td>84.13</td>
                </tr>
                <tr>
                    <td>20</td>
                    <td>60000</td>
                    <td>100/50/10</td>
                    <td>0.5</td>
                    <td>False</td>
                    <td>65.16</td>
                </tr>
            </tbody>
        </table>
        <div class="project-subsubtitle">
            Visualising the Effects of Gradient Descent with Momentum
        </div>
        <div class="project-text">
            Comparing two identical networks, one with momentum and one without, its obvious that the network with momentum converges much faster and smoothly to the optimal solution.
        </div>
        <img class="image" src="./assets/neuralnetproject/effects_of_momentum.png" alt="Momentum Comparison"></img>
</body>